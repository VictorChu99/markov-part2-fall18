This is the analysis for Markov Part 2, Fall 2018

Include your name and net id here. Answer the questions from the assignment 
that are reproduced below

Victor Chu
vic4
10.19.18

(1) Determine (from running Benchmark.java) how long it takes for 
BaseMarkov to generate 2,000, 4,000, 8,000, 16,000, and 32,000 
random characters using the default file and an order 5 Markov Model. 
Include these timings in your report. 

Starting tests

time	source	#chars
0.268	507914	2000
0.525	507914	4000
1.024	507914	8000
2.447	507914	16000
4.698	507914	32000

0.619	507914	4096
1.168	1015828	4096
1.987	1523742	4096
2.707	2031656	4096
3.503	2539570	4096
4.087	3047484	4096
4.716	3555398	4096
5.403	4063312	4096
5.948	4571226	4096
6.417	5079140	4096

Finished tests

Source code is 496,768, but I got 507,914. Piazza post addressed this issue. 
TA said it was okay as well


The program also generates 4,096 characters using texts that increase in 
size from 496,768 characters to 4,967,680 characters (10 times the number).  
Do you the timings support the O(NT) analysis for BaseMarkov?

Yes. When we multiplied the source by 10, the time roughly increased by a factor 
of 10 as well. These claims support O(N) complexity. Since we are increasing the characters
by a factor of 10, the program has to go through each of those characters again. For example,
if you doubled the characters, you would increase time by roughly a factor of 2. As you 
increase the amount of characters, this complexity gets longer. My benchmark tests support this
as O(NT) as seen by the time going from 0.619 to 6.417 which is roughly 10. It isn't exactly 10 times longer,
because each computer has its own different schemes that may affect run time. 
 This is O(NT) because we first scan the which is O(N), but then rescan for getFollows() is O(T),
 making it O(NT). Important to note for BaseMarkov but as the order gets larger, this 
 will get more efficient because we are doing less iterations. 



(2) Determine (from running Benchmark.java) how long it takes for 
EfficientMarkov to generate 2,000, 4,000, 8,000, 16,000, and 32,000 
random characters using the default file and an order 5 Markov Model. 
Include these timings in your report. 

Starting tests

time	source	#chars
0.124	507914	2000
0.103	507914	4000
0.117	507914	8000
0.095	507914	16000
0.118	507914	32000

0.075	507914	4096
0.182	1015828	4096
0.305	1523742	4096
0.416	2031656	4096
0.462	2539570	4096
0.579	3047484	4096
0.722	3555398	4096
1.454	4063312	4096
1.648	4571226	4096
1.809	5079140	4096

Finished tests


The program also generates 4,096 characters using texts that increase in 
size from 496,768 characters to 4,967,680 characters (10 times the number).  
Do you the timings support the O(N+T) analysis for EfficientMarkov?

Yes. As seen above, T has less of an effect on the total runtime, 
because we are adding it. When T is small, it has somewhat of an impact,
because 10 might be a large number, relative to N. However,
when N gets huge,T doesn't matter as much. As seen with the runtime between BaseMarkov
and EfficientMarkov, initially, T has somewhat of an impact increasing runtime just slightly
for EfficientMarkov. However, as N gets larger, this 'T' doesn't matter
as much, and EfficientMarkov is much quicker than BaseMarkov which has a runtime of O(NT). 
This is due to the fact that O(NT) is much larger, because when your N gets large, 
you multiply that by a factor of 10 making it even larger. O(N+T) is reasonable, 
because the getFollows is a O(T) operation in this case(actually O(1 but called T times),
and scanning the text is O(N), therefore, O(N+T). Also, EfficientMarkov, 
was noticeably faster than BaseMarkov because it implemented a Map with key,value pairs.




(3)The tests in the class Benchmark use an order-5 Markov Model. 
Run tests that you think are appropriate to determine if the order of the 
Markov Model has a significant impact on the running time for BaseMarkov. 
Explain your reasoning.

The orders I used were order-400,000 and order-1000 as shown below. Order has a 
significant impact on the overall runtime in this program. Order 4000000 was super 
fast because, we are essentially blocking more out of the text. This means less iterations. 
However, when we have order 1000, it takes longer because we are having to make 
more iterations. This results in a longer runtime as shown below. For example, 
if we had a text of abababababababababa... so on so forth repeating this pattern, as the order
got larger, we would get faster because of less iterations. The relative size of the program may not be
changing, but the order-markov we are using is, which affects runtime. 

ORDER 400,000

Starting tests

time	source	#chars
0.001	507914	2000
0.000	507914	4000
0.001	507914	8000
0.001	507914	16000
0.001	507914	32000

0.001	507914	4096
0.001	1015828	4096
0.001	1523742	4096
0.001	2031656	4096
0.001	2539570	4096
0.001	3047484	4096
0.001	3555398	4096
0.001	4063312	4096
0.001	4571226	4096
0.000	5079140	4096

Finished tests

ORDER 1000

Starting tests

time	source	#chars
0.753	507914	2000
0.763	507914	4000
0.775	507914	8000
0.749	507914	16000
0.769	507914	32000

0.618	507914	4096
1.582	1015828	4096
2.919	1523742	4096
4.168	2031656	4096
5.592	2539570	4096
8.568	3047484	4096
11.460	3555398	4096
10.175	4063312	4096
15.856	4571226	4096
15.440	5079140	4096
17.907	5587054	4096
15.571	6094968	4096
18.658	6602882	4096
20.483	7110796	4096
22.509	7618710	4096
20.166	8126624	4096
21.066	8634538	4096
22.606	9142452	4096
26.407	9650366	4096
36.875	10158280	4096
26.513	10666194	4096
27.921	11174108	4096
30.202	11682022	4096
31.746	12189936	4096
43.268	12697850	4096




























